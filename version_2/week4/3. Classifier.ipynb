{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematically Improving RAG - Week 4\n",
    "\n",
    "## Applying Our Topics\n",
    "\n",
    "In the previous notebook, we used topic modelling to generate topics from our dataset. Through that we realised that there were a significant number of queries related to refunds that we were performing poorly on. We also learnt that there were broadly four different types of queries when it came to refunds\n",
    "\n",
    "1. Queries about the refund status of their order\n",
    "2. Queries about whether their order was eligible for a refund\n",
    "3. Queries about wanting a refund instead of store credit\n",
    "4. Queries to ask for help because the store didn't respond to their requests\n",
    "\n",
    "We want to take these insights and build out a classifier that can assign these four types of queries to new queries so that we can monitor and improve our performance on them over time. We can do so using a simple `.yaml` file that contains the categories we've identified over time.\n",
    "\n",
    "We've defined a few categories in our `categories.yml` file that we'll use to classify our queries. This makes it easy to add new categories in the future and allow other members of the team to add new categories as they see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-defined categories to classify queries\n",
    "\n",
    "We'll start by loading in our categories from the `categories.yml` file. We'll use instructor and pydantic here to handle the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Return/Refund Process',\n",
       "  'description': 'User is asking how to initiate a return or refund for their order'},\n",
       " {'title': 'Return/Refund Status',\n",
       "  'description': 'User is inquiring about the status of their return or refund'},\n",
       " {'title': 'Eligibility',\n",
       "  'description': 'User wants to know if their order is eligible for a return or refund'},\n",
       " {'title': 'Policy Details',\n",
       "  'description': \"User is asking about the company's return and refund policies\"}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from pydantic import BaseModel, ValidationInfo, field_validator\n",
    "\n",
    "\n",
    "class QuestionType(BaseModel):\n",
    "    types: list[str]\n",
    "\n",
    "    @field_validator(\"types\")\n",
    "    def validate_categories(cls, v: list[str], info: ValidationInfo):\n",
    "        context = info.context\n",
    "        if not context:\n",
    "            raise ValueError(\"No context provided\")\n",
    "        question_types = context.get(\"question_types\")\n",
    "        if not question_types:\n",
    "            raise ValueError(\"No question types provided\")\n",
    "\n",
    "        question_type_names = [question_type[\"title\"] for question_type in question_types]\n",
    "\n",
    "        for question_type in v:\n",
    "            if question_type not in question_type_names:\n",
    "                raise ValueError(\n",
    "                    f\"Question type {question_type} not found in original list of question types\"\n",
    "                )\n",
    "        return v\n",
    "\n",
    "\n",
    "yaml_config = yaml.load(open(\"categories.yml\", \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "question_types = []\n",
    "for question_type_category in yaml_config[\"question_type\"]:\n",
    "    question_types.extend(yaml_config[\"question_type\"][question_type_category])\n",
    "\n",
    "question_types[:4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to then load in these categories into our prompt. `instructor` allows us to use the same variable in our template formatting and our validation, making it easy to validate that our generated categories are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuestionType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">types</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Return/Refund Status'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQuestionType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtypes\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Return/Refund Status'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuestionType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">types</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Store Response'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Warranty Information'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Others'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQuestionType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtypes\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Store Response'\u001b[0m, \u001b[32m'Warranty Information'\u001b[0m, \u001b[32m'Others'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuestionType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">types</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Eligibility'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Policy Details'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Shipping Information'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQuestionType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtypes\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Eligibility'\u001b[0m, \u001b[32m'Policy Details'\u001b[0m, \u001b[32m'Shipping Information'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuestionType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">types</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Others'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQuestionType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtypes\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Others'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "import instructor\n",
    "from rich import print\n",
    "\n",
    "\n",
    "client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "\n",
    "def classify_query(query: str) -> QuestionType:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "            Breakdown the folowing query into the relevant question types. Select all that apply to the query itself.\n",
    "            \n",
    "            Question Types:\n",
    "            {% for question_type in question_types %}\n",
    "            - {{ question_type.title }} : {{ question_type.description }}\n",
    "            {% endfor %}\n",
    "            \n",
    "            Make sure to only return the categories that are in the list of categories.\n",
    "            \"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        response_model=QuestionType,\n",
    "        context={\"question_types\": question_types},\n",
    "    )\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"What's the status of my refund?\",\n",
    "    \"Nike isn't responding to me at all despite the broken shoebag, does Klarna provide any warranty support?\",\n",
    "    \"Will Klarna cover the cost of returning my order? I'm out of state and I need to ship it to New York\",\n",
    "    \"How many litres of milk does a cow produce in a day?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(classify_query(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run this in production so that we can monitor and check our performance on the different question types over time.\n",
    "\n",
    "This is especially true in this case for the refund categories where we're performing poorly. We can make this easier by building dashboards where we can \n",
    "\n",
    "- Track the distribution of these question types over time\n",
    "- Track the % of others to detect drift in our system\n",
    "- Track satisfaction scores and volume for each query type\n",
    "\n",
    "and so on.\n",
    "\n",
    "By investing in this analysis on a consistent basis, we can make sure that we're able to improve our system time by identifying and prioritising the queries that we need to improve on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
