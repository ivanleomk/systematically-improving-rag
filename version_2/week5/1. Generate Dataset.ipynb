{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Systematically Improving Your RAG Application\n",
    "\n",
    "## Why Use Language Models for Metadata Generation?\n",
    "\n",
    "We need to return results that are both relevant and accurate based on the user's query. This requires us to use effective filtering mechanisms. We want to use language models to generate these metadata fields because they are fast and cheap. Using structured extraction, we can generate metadata fields that are more accurate and consistent than human annotators.\n",
    "\n",
    "Why does this matter? Imagine a query from a user - \"I'm looking for a black t-shirt under $50 made of cotton.\n",
    "\n",
    "This seemingly simple request contains multiple filtering criteria:\n",
    "- Color (black)\n",
    "- Price range (under $50)\n",
    "- Material preference (cotton)\n",
    "\n",
    "Additionally, the system might have user-specific metadata like:\n",
    "- Typical size preferences (S/M)\n",
    "- Preferred brands (e.g., Zara)\n",
    "- Shopping history\n",
    "\n",
    "In this three part notebook, we'll showcase how we can improve retrieval performance by using LLM-generated metadata and query understanding. We'll do so in three parts\n",
    "\n",
    "1. **Dataset Creation**: Using the `irow/ClothingControlV2` dataset, we'll generate a dataset of items that are labelled with a taxonomy that we've defined ahead of time. \n",
    "2. **Query Generation**: We'll then generate synthetic queries that require the use of metadata filtering to answer.\n",
    "3. **Text-2-SQL**: Finally, we'll see how we can augment our retrieval with some `text-2-sql` techniques to improve the ability of our model to answer more complex queries around size, color and stock.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating our Dataset\n",
    "\n",
    "In this portion, we'll generate a dataset that mimics a e-commerce company's product catalog. In order to do so, we'll be extracting out item data from images using `gpt-4o` and then using a taxonomy to classify the items.\n",
    "\n",
    "## Loading in our Taxonomy\n",
    "\n",
    "E-Commerce companies use what's called a taxonomy to classify their products. In our case, we've chosen the following fields\n",
    "\n",
    "- `category` : This is a high level category such as Men's, Women's, Unisex, etc.\n",
    "- `subcategory` : This is a more specific category such as T-Shirts, Blouses that are under a specific category\n",
    "- `types` : These are more specific product types such as Crew Neck T-Shirt, V-Neck T-Shirt, etc.\n",
    "- `attributes` These are attributes that are specific to the items that have that specific category, subcategory and type combination.\n",
    "- `common_attributes` These are attributes that are common to all items in our database such as sizes and colors in stock\n",
    "\n",
    "We want to define a taxonomy ahead of time for three main reasons\n",
    "\n",
    "1. **Consistency** : By having a consistent taxonomy, we can ensure that we only generate data on items that fall within our taxonomy. \n",
    "2. **Filtering** : It makes it easy for us to map user queries to a set of known metadata fields which we can use to filter our retrieved items down the line.\n",
    "3. **Non-Technical Help**: By using a human-readable format like yaml, we can ask members of our team that aren't technical to help define the proper taxonomy. You can implement this too using a no-sql database to store raw taxonomies or configs but we've chosen to keep it simply for now.\n",
    "\n",
    "Let's now read in our taxonomy and see how we can enforce these fields. We'll define some simple Pydantic models to make it easy for us to work with the yaml data\n",
    "\n",
    "We've defined a `progress_taxonomy_file` function to help us process the yaml file and convert it into a dictionary. We use `pydantic` and `instructor` to help make sure that our LLM generated metadata conforms to our taxonomy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['taxonomy_map', 'occasions', 'materials', 'common_attributes', 'taxonomy'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers import process_taxonomy_file\n",
    "\n",
    "taxonomy_data = process_taxonomy_file(\"taxonomy.yml\")\n",
    "\n",
    "taxonomy_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Size', 'Color', 'Material', 'Pattern', 'Occasion'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomy_data[\"common_attributes\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import model_validator, ValidationInfo, BaseModel\n",
    "\n",
    "\n",
    "class ItemAttribute(BaseModel):\n",
    "    name: str\n",
    "    value: str\n",
    "\n",
    "\n",
    "class ItemMetadata(BaseModel):\n",
    "    title: str\n",
    "    brand: str\n",
    "    description:str\n",
    "    category: str\n",
    "    subcategory: str\n",
    "    product_type: str\n",
    "    attributes: list[ItemAttribute]\n",
    "    material: str\n",
    "    pattern: str\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_material_and_pattern(self, info: ValidationInfo):\n",
    "        context = info.context\n",
    "        if not context or not context[\"taxonomy_data\"]:\n",
    "            raise ValueError(\"Taxonomy data is required for validation\")\n",
    "\n",
    "        if self.pattern not in context[\"taxonomy_data\"][\"common_attributes\"][\"Pattern\"]:\n",
    "            raise ValueError(\n",
    "                f\"Pattern {self.pattern} is not a valid pattern. Valid patterns are {context['taxonomy_data']['common_attributes']['Pattern']}\"\n",
    "            )\n",
    "\n",
    "        if self.material not in context[\"taxonomy_data\"][\"common_attributes\"][\"Material\"]:\n",
    "            raise ValueError(\n",
    "                f\"Material {self.material} is not a valid material. Valid materials are {context['taxonomy_data']['common_attributes']['Material']}\"\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_category_and_attributes(self, info: ValidationInfo):\n",
    "        context = info.context\n",
    "        if not context or not context[\"taxonomy_data\"]:\n",
    "            raise ValueError(\"Taxonomy data is required for validation\")\n",
    "\n",
    "        taxonomy_map = context[\"taxonomy_data\"][\"taxonomy_map\"]\n",
    "\n",
    "        # 1. Validate category\n",
    "        if self.category not in taxonomy_map:\n",
    "            raise ValueError(\n",
    "                f\"Category {self.category} is not valid. Valid categories are {list(taxonomy_map.keys())}\"\n",
    "            )\n",
    "\n",
    "        # 2. Validate subcategory\n",
    "        if self.subcategory not in taxonomy_map[self.category]:\n",
    "            raise ValueError(\n",
    "                f\"Subcategory {self.subcategory} does not exist under category {self.category}\"\n",
    "            )\n",
    "\n",
    "        subcategory_data = taxonomy_map[self.category][self.subcategory]\n",
    "\n",
    "        # 3. Validate product type\n",
    "        if self.product_type not in subcategory_data[\"product_type\"]:\n",
    "            raise ValueError(\n",
    "                f\"Product type {self.product_type} is not valid for subcategory {self.subcategory}. Valid types are {subcategory_data['product_type']}\"\n",
    "            )\n",
    "\n",
    "        # 4. Validate attributes\n",
    "        for attr in self.attributes:\n",
    "            if attr.name not in subcategory_data[\"attributes\"]:\n",
    "                raise ValueError(\n",
    "                    f\"Attribute {attr.name} is not valid for subcategory {self.subcategory}. Valid attributes are {list(subcategory_data['attributes'].keys())}\"\n",
    "                )\n",
    "\n",
    "            if attr.value not in subcategory_data[\"attributes\"][attr.name]:\n",
    "                raise ValueError(\n",
    "                    f\"Value {attr.value} is not valid for attribute {attr.name}. Valid values are {subcategory_data['attributes'][attr.name]}\"\n",
    "                )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Item Data from Images\n",
    "\n",
    "The `irow/ClothingControlV2` dataset contains images of clothing items that are generated using a control net. It doesn't have any product data and so we'll extract out the item data from the images. \n",
    "\n",
    "We use `instructor` here to help us extract the item data from the images. Note here that we're rendering the entire yml file as context. We want to do so for two reasons\n",
    "\n",
    "1. Firstly, providing all of the possible choices allows the model more flexibility in deciding what the right metadata fields are\n",
    "2. Secondly, if we have a large taxonomy, we can leverage techniques like prompt caching to save on costs. By ensuring that the initial portion of the prompt is the same, we can leverage caching to speed up the extraction process.\n",
    "\n",
    "We'll be using `gpt-4o` here for the extraction since it supports multimodal inputs ( in this case images )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = [item for item in load_dataset(\"irow/ClothingControlV2\",streaming=True)[\"train\"].take(2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ItemMetadata(title='Lace Detail Sleeveless Top', brand='H&M', description='Elevate your wardrobe with this elegant sleeveless top featuring intricate lace detailing at the neckline. Perfect for a chic daytime look or a night out, this versatile piece combines comfort and style effortlessly.', category='Women', subcategory='Tops', product_type='Tank Tops', attributes=[ItemAttribute(name='Sleeve Length', value='Sleeveless'), ItemAttribute(name='Neckline', value='Crew Neck'), ItemAttribute(name='Fit', value='Regular')], material='Cotton', pattern='Solid')]\n"
     ]
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "import tempfile\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "with open(\"taxonomy.yml\", \"r\") as f:\n",
    "    taxonomy = f.read()\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as f:\n",
    "    ds[0][\"image\"].save(f.name)\n",
    "    items = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert at extracting item data from images. Extract 1-2 items seen in the images based on the taxonomy provided. Here are the categories, subcategories, types and attributes that you can choose from: {{ taxonomy_data['taxonomy_map'] }}\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    \"Here is the image, choose a brand that likely sells these items - choose a real brand that exists in real life and make up a name if that's not possible. Also generate a short description of 1-2 sentences of the item that would be suitable for an e-commerce website\",\n",
    "                    instructor.Image.from_path(f.name),\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        response_model=list[ItemMetadata],\n",
    "        context={\n",
    "            \"taxonomy_data\": taxonomy_data\n",
    "        },\n",
    "        \n",
    "    )\n",
    "\n",
    "    print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `gpt-4o` was able to extract out the item data from the image and that the metadata fields conform to the taxonomy that we've defined. We've extracted out a list of items that are in the image and mapped them to a category, subcategory and other metadata fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "import tempfile\n",
    "from asyncio import Semaphore, timeout\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from tqdm.asyncio import tqdm_asyncio as asyncio\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))\n",
    "async def generate_dataset_label(\n",
    "    dataset_item: dict, client: instructor.AsyncInstructor, sem: Semaphore, taxonomy_data: dict\n",
    "):\n",
    "    async with sem, timeout(30):\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as f:\n",
    "            dataset_item[\"image\"].save(f.name)\n",
    "            items = await client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an expert at extracting item data from images. Extract 1-2 items seen in the images based on the taxonomy provided. Here are the categories, subcategories, types and attributes that you can choose from: {{ taxonomy_data['taxonomy_map'] }}\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            \"Here is the image, choose a brand that likely sells these items - choose a real brand that exists in real life and make up a name if that's not possible. Also generate a short description of 1-2 sentences of the item that would be suitable for an e-commerce website\",\n",
    "                            instructor.Image.from_path(f.name),\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "                response_model=list[ItemMetadata],\n",
    "                context={\n",
    "                    \"taxonomy_data\": taxonomy_data\n",
    "                },\n",
    "                \n",
    "            )\n",
    "\n",
    "            return [\n",
    "                {\"image\": dataset_item[\"image\"], \"metadata\": item} for item in items\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:19<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "sem = Semaphore(15)\n",
    "n_rows = 150\n",
    "\n",
    "ds = [item for item in load_dataset(\"irow/ClothingControlV2\",streaming=True)[\"train\"].take(n_rows)]\n",
    "results = await asyncio.gather(\n",
    "    *[generate_dataset_label(ds_row, client, sem, taxonomy_data) for ds_row in ds]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can create a dataset, we need to flatten the list of items we've extracted from the images. We'll also flatten the metadata fields so that we can create a dataset with the correct schema. Since our attributes are a nested list of objects, we'll convert them to a json string for convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Lace Detail Sleeveless Top',\n",
       " 'brand': 'H&M',\n",
       " 'description': \"Elevate your casual wardrobe with this elegant sleeveless top featuring intricate lace detailing at the neckline. Perfect for both day and night, it's crafted from a soft, breathable fabric for all-day comfort.\",\n",
       " 'category': 'Women',\n",
       " 'subcategory': 'Tops',\n",
       " 'product_type': 'Tank Tops',\n",
       " 'attributes': [{'name': 'Sleeve Length', 'value': 'Sleeveless'},\n",
       "  {'name': 'Neckline', 'value': 'Crew Neck'}],\n",
       " 'material': 'Cotton',\n",
       " 'pattern': 'Solid'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten results list of lists into a single list\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "flattened_results[0][\"metadata\"].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=768x1024>,\n",
       " 'title': 'Lace Detail Sleeveless Top',\n",
       " 'brand': 'H&M',\n",
       " 'description': \"Elevate your casual wardrobe with this elegant sleeveless top featuring intricate lace detailing at the neckline. Perfect for both day and night, it's crafted from a soft, breathable fabric for all-day comfort.\",\n",
       " 'category': 'Women',\n",
       " 'subcategory': 'Tops',\n",
       " 'product_type': 'Tank Tops',\n",
       " 'attributes': '[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveless\"}, {\"name\": \"Neckline\", \"value\": \"Crew Neck\"}]',\n",
       " 'material': 'Cotton',\n",
       " 'pattern': 'Solid',\n",
       " 'id': 1,\n",
       " 'price': 181.04}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def flatten_item(item: dict, id: int):\n",
    "    flattened_item = {\"image\": item[\"image\"], **item[\"metadata\"].model_dump()}\n",
    "\n",
    "    return {\n",
    "        **flattened_item,\n",
    "        \"id\": id,\n",
    "        \"price\": round(random.uniform(10.0, 400.0),2),\n",
    "\n",
    "        \"attributes\": json.dumps(flattened_item[\"attributes\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "hf_dataset_items = [flatten_item(item, id+1) for id, item in enumerate(flattened_results)]\n",
    "hf_dataset_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 191/191 [00:00<00:00, 21290.32 examples/s]/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 317.61ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ivanleomk/ecommerce-taxonomy/commit/f404c96bf9e1ec0d3de7026312f5bcd36f18ceef', commit_message='Upload dataset', commit_description='', oid='f404c96bf9e1ec0d3de7026312f5bcd36f18ceef', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ivanleomk/ecommerce-taxonomy', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ivanleomk/ecommerce-taxonomy'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's create a new HF dataset with the labelled data so that we have it stored\n",
    "# Convert to HuggingFace Dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create HF dataset\n",
    "dataset = Dataset.from_list(hf_dataset_items)\n",
    "dataset.push_to_hub(\"ivanleomk/ecommerce-taxonomy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple notebook, we've seen the value of using LLMs to generate metadata for a RAG application. From a simple image, we were able to bootstrap a dataset with item metadata that allows us to return queries that are not only relevant but also accurate. \n",
    "\n",
    "In the next notebook, we'll generate some synthetic questions that require the use of complex metadata filtering to answer. We'll then see how we can use our LLM-generated metadata to improve the retrieval of these questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
